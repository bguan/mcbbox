{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pylightning_frcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Object Detection using COCO Dataset\n",
    "\n",
    "## Pytorch Lightning & Torch Vision\n",
    "\n",
    "Let's start by using what's already in Torch Vision, see [example](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
    "To force some learning, I shall attempt to port the example to [Pytorch-Lightning](https://github.com/PyTorchLightning/pytorch-lightning) as I've read that it removes a lot of boiler plate code and standardized Pytorch usage. As well as make advanced features like gradient accumulation and multi-GPU multi-node training simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import json, os, requests, sys, tarfile, torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.utils import io\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import *    \n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Python ver {sys.version}, torch {torch.__version__}, torchvision {torchvision.__version__}, pytorch_lightning {pl.__version__}\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Sample of COCO Data\n",
    "\n",
    "The full COCO Dataset is huge (~50GB?). For my self education exploring object detection, with the intention of using pretrained model in transfer learning, it is not practical to deal with dataset this big as my first project.  Luckily, the kind folks at [FastAI](https://fast.ai) have prepared some convenient subsets, the medium size 3GB https://s3.amazonaws.com/fast-ai-coco/coco_sample.tgz seems like a good candidate.  The 800KB \"http://files.fast.ai/data/examples/coco_tiny.tgz\" on the other hand seems way too small, thus may not have enough data for adequate training.\n",
    "\n",
    "However, to allow faster iteration, let's start with the tiny dataset just to test drive the whole process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def fetch_data(url:str, datadir: Path, tgt_fname:str, chunk_size:int=8*1024):\n",
    "    dest = datadir/tgt_fname\n",
    "    print(f\"Downloading from {url} to {dest}...\")\n",
    "    with requests.get(url, stream=True, timeout=10) as response:\n",
    "        content_len = int(response.headers['content-length'])\n",
    "        with open(dest, 'wb') as f:\n",
    "            with tqdm(total=content_len) as pbar:\n",
    "                nbytes = 0\n",
    "                num_chunks = 0\n",
    "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                    chunk_len = len(chunk)\n",
    "                    nbytes += chunk_len\n",
    "                    num_chunks += 1\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_len)\n",
    "\n",
    "    with tarfile.open(dest, 'r') as tar:\n",
    "        extracted = []\n",
    "        for item in tar:\n",
    "            tar.extract(item, datadir)\n",
    "            extracted.append(item.name)\n",
    "\n",
    "    print(f\"Downloaded {nbytes} from {url} to {dest}, extracted in {datadir}: {extracted[:3]},...,{extracted[-3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "froot = \"coco_tiny\"\n",
    "fname = f\"{froot}.tgz\"\n",
    "url = f\"http://files.fast.ai/data/examples/{fname}\"\n",
    "\n",
    "# If using the bigger Coco subset, use these values\n",
    "# froot = \"coco_sample\"\n",
    "# fname = f\"{froot}.tgz\"\n",
    "# url = f\"https://s3.amazonaws.com/fast-ai-coco/{fname}\"\n",
    "\n",
    "datadir = Path(\"workspace\")\n",
    "fetch_data(url, datadir, fname, chunk_size=1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Annotations\n",
    "\n",
    "Let's load and inspect the annotation file that comes with the coco tiny dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "json_fname = datadir/froot/'train.json'\n",
    "with open(json_fname, 'r') as json_f:\n",
    "    train_json = json.load(json_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json['categories'], train_json['images'][0], [a for a in train_json['annotations'] if a['image_id']==train_json['images'][0]['id'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digest the Dataset for useful Stats\n",
    "\n",
    "Do some basic analysis of the data to get numbers like total images, boxes, and average box count per image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CocoDatasetStats():\n",
    "    # num_cats\n",
    "    # num_imgs\n",
    "    # num_bboxs\n",
    "    # cat2name\n",
    "    # lbl2cat\n",
    "    # cat2lbl\n",
    "    # img2fname\n",
    "    # imgs\n",
    "    # img2cat2bs\n",
    "    # cat2ibs\n",
    "    # avg_ncats_per_img\n",
    "    # avg_nboxs_per_img\n",
    "    # avg_nboxs_per_cat\n",
    "    # chn_means\n",
    "    # chn_stds\n",
    "    def __init__(self, ann:dict):\n",
    "        self.num_cats = len(ann['categories'])\n",
    "        self.num_imgs = len(ann['images'])\n",
    "        self.num_bboxs = len(ann['annotations'])\n",
    "        \n",
    "        # build cat id to name, assign FRCNN \n",
    "        self.cat2name = { c['id']: c['name'] for c in ann['categories'] }\n",
    "        \n",
    "        # expected labels w 0 = background\n",
    "        self.lbl2cat = { l+1: c for l, (c, n) in enumerate(self.cat2name.items()) }\n",
    "        self.cat2lbl = { c: l+1 for l, (c, n) in enumerate(self.cat2name.items()) }\n",
    "        self.lbl2cat[0] = (0, 'background')\n",
    "        self.cat2lbl[0] = 0\n",
    "\n",
    "        # img_id to file map\n",
    "        self.img2fname = { img['id']:img['file_name'] for img in ann['images'] }\n",
    "        self.imgs = [ { 'id':img_id, 'file_name':img_fname } for (img_id, img_fname) in self.img2fname.items() ]\n",
    "\n",
    "        # build up 2 maps for later analysis\n",
    "        self.img2cat2ibs = defaultdict(lambda: defaultdict(lambda:set()))\n",
    "        self.img2liibs = defaultdict(lambda:[])\n",
    "        self.cat2iibs = defaultdict(lambda:set())\n",
    "        anno_id = 0\n",
    "        for a in ann['annotations']:\n",
    "            img_id = a['image_id']\n",
    "            cat_id = a['category_id']\n",
    "            c2ibs_for_img = self.img2cat2ibs[img_id]\n",
    "            (x, y, w, h) = a['bbox']\n",
    "            ib = (anno_id, x, y, w, h) \n",
    "            iib = (img_id, *ib)\n",
    "            liib = (cat_id, *iib)\n",
    "            c2ibs_for_img[cat_id].add(ib)\n",
    "            self.cat2iibs[cat_id].add(iib)\n",
    "            self.img2cat2ibs[img_id] = c2ibs_for_img\n",
    "            self.img2liibs[img_id].append(liib)\n",
    "            anno_id +=1\n",
    "\n",
    "        acc_ncats_per_img = 0.0\n",
    "        acc_nboxs_per_img = 0.0\n",
    "        for img_id, c2ibs in self.img2cat2ibs.items():\n",
    "            acc_ncats_per_img += len(c2ibs)\n",
    "            for cat_id, ibs in c2ibs.items():\n",
    "                acc_nboxs_per_img += len(ibs)\n",
    "\n",
    "        self.avg_ncats_per_img = acc_ncats_per_img/self.num_imgs\n",
    "        self.avg_nboxs_per_img = acc_nboxs_per_img/self.num_imgs\n",
    "\n",
    "        acc_nboxs_per_cat = 0.0\n",
    "        for cat_id, iibs in self.cat2iibs.items():\n",
    "            acc_nboxs_per_cat += len(iibs)\n",
    "\n",
    "        self.avg_nboxs_per_cat = acc_nboxs_per_cat/self.num_cats\n",
    "        \n",
    "        # compute Images per channel means and std deviation using Welford’s method\n",
    "        \n",
    "        n = 0\n",
    "        mean = np.zeros((3,))\n",
    "        M2 = np.zeros((3,))\n",
    "\n",
    "        for img in self.imgs:\n",
    "            fname = img['file_name']\n",
    "            n = n + 1\n",
    "            img = Image.open(datadir/froot/'train'/fname)\n",
    "            ia = np.asarray(img)\n",
    "            x = np.mean(ia,axis=(0,1))\n",
    "            delta = x - mean\n",
    "            mean = mean + delta/n\n",
    "            M2 = M2 + delta*(x - mean)\n",
    "\n",
    "        variance = M2/(n - 1)\n",
    "        \n",
    "        self.chn_means = mean\n",
    "        self.chn_stds = np.sqrt(variance)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "stats = CocoDatasetStats(train_json)\n",
    "\n",
    "print(\n",
    "    f\"Categories {stats.num_cats}, Images {stats.num_imgs}, Boxes {stats.num_bboxs}, \"\n",
    "    f\"avg cats/img {stats.avg_ncats_per_img:.1f}, avg boxs/img {stats.avg_nboxs_per_img:.1f}, avg boxs/cat {stats.avg_nboxs_per_cat:.1f}.\")\n",
    "\n",
    "print(f\"Image means by channel {stats.chn_means}, std.dev by channel {stats.chn_stds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out number of boxes per category to see if the distribution is not too unbalanced..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ (cid, stats.cat2name[cid]): len(iibs) for cid, iibs in stats.cat2iibs.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, seems like a lot of chairs and books, but vases, tvs, couches and remotes are roughly 10x less. This may be a problem. \n",
    "But we have enough to test drive the training pipeline. \n",
    "\n",
    "To make it more balanced, may be I can combine vase+tv+couch+remote into a new category 'others' later?\n",
    "\n",
    "Anyway, we can also switch to a bigger dataset (e.g. the 3GB coco-sample) once we are happy with code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Images\n",
    "\n",
    "Let's look at an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pos = 0\n",
    "img_id = train_json['images'][img_pos]['id']\n",
    "cat2ibs = stats.img2cat2ibs[img_id]\n",
    "img_fname = stats.img2fname[img_id]\n",
    "img = Image.open(datadir/froot/'train'/img_fname)\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.imshow(img)\n",
    "img_id, img_fname, cat2ibs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlay Boxes and Labels from Annotation\n",
    "\n",
    "Let's overlay bounding boxes and labels over the image to confirm our understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def bbox_to_rect(ibbox, color):\n",
    "    return plt.Rectangle(\n",
    "        xy=(ibbox[1], ibbox[2]), width=ibbox[3], height=ibbox[4],\n",
    "        fill=False, edgecolor=color, linewidth=2)\n",
    "\n",
    "def label_for_bbox(ibbox, label, color):\n",
    "    return plt.text(ibbox[1], ibbox[2], f\"{ibbox[0]}.{label}\", color=color, fontsize=16)\n",
    "\n",
    "def overlay_img_bbox(img:Image, cat2ibs: dict, cat2name: dict):\n",
    "    cat2color = { cid: cname for (cid, cname) in zip(cat2ibs.keys(), mcolors.TABLEAU_COLORS.keys()) }\n",
    "    fig = plt.figure(figsize=(16,10))\n",
    "    fig = plt.imshow(img)\n",
    "    for cid, ibs in cat2ibs.items():\n",
    "        for ib in ibs:\n",
    "            label_for_bbox(ib, cat2name[cid], cat2color[cid])\n",
    "            fig.axes.add_patch(bbox_to_rect(ib, cat2color[cid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick some random image to test drive the box overlay code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 129739\n",
    "img = Image.open(datadir/froot/'train'/stats.img2fname[img_id])\n",
    "overlay_img_bbox(img, stats.img2cat2ibs[img_id], stats.cat2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Data Loading Logic using Pytorch-Lightning\n",
    "\n",
    "\n",
    "Now we need to define a DataModule to encapsulate all the data loading logic. At first I thought I can reuse CocoDetect() from torchvision but it and downstream cocoapi expects json annotation file to be of this [format](https://cocodataset.org/#format-data):\n",
    "```\n",
    "annotation{\n",
    "    \"id\": int,\n",
    "    \"image_id\": int,\n",
    "    \"category_id\": int,\n",
    "    \"segmentation\": RLE or [polygon],\n",
    "    \"area\": float,\n",
    "    \"bbox\": [x,y,width,height],\n",
    "    \"iscrowd\": 0 or 1,\n",
    "}\n",
    "```\n",
    "\n",
    "Tiny Coco's train.json file only has a subset of the above fields:\n",
    "```\n",
    "“Annotations”: [\n",
    "    {\n",
    "      \"image_id\": 542959,\n",
    "      \"bbox\": [\n",
    "        32.52,\n",
    "        86.34,\n",
    "        8.53,\n",
    "        9.41\n",
    "      ],\n",
    "      \"category_id\": 62\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "``` \n",
    "\n",
    "Thus we will need to make a Dataset to handle it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "    \n",
    "class SubCocoDataset(torchvision.datasets.VisionDataset):\n",
    "    \"\"\"\n",
    "    Simulate what torchvision.CocoDetect() returns for target given fastai's coco subsets\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        stats (CocoDatasetStats): \n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, stats, transform=None, target_transform=None, transforms=None):\n",
    "        super(SubCocoDataset, self).__init__(root, transforms, transform, target_transform)\n",
    "        self.stats = stats\n",
    "        self.img_ids = list(stats.img2fname.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        img_id = self.img_ids[index] if index < len(self.img_ids) else 0\n",
    "        img_fname = self.stats.img2fname.get(img_id, None)\n",
    "        if img_id == None or img_fname ==None:\n",
    "            print(f\"__getitem__({index}): got img_id {img_id}, img_fname {img_fname}\")\n",
    "            return (None, None)\n",
    "        \n",
    "        img = Image.open(os.path.join(self.root, img_fname)).convert('RGB')\n",
    "        target = { \"boxes\": [], \"labels\": [], \"image_id\": None, \"area\": [], \"iscrowd\": 0, \"ids\": [] }\n",
    "        count = 0\n",
    "        liibs = self.stats.img2liibs.get(img_id,[])\n",
    "        for cat_id, img_id, anno_id, x, y, w, h in liibs:\n",
    "            count += 1\n",
    "            target[\"boxes\"].append([x, y, x+w, y+h])\n",
    "            target[\"labels\"].append(self.stats.cat2lbl[cat_id])\n",
    "            target[\"image_id\"] = img_id\n",
    "            target[\"area\"].append(w*h)\n",
    "            target[\"ids\"].append(anno_id)\n",
    "\n",
    "        for k, v in target.items():\n",
    "            target[k] = torch.tensor(v)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        else:\n",
    "            if self.transform is not None: img = self.transform(img)\n",
    "            if self.target_transform is not None: target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.stats.num_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the SubCocoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([ transforms.ToTensor() ])\n",
    "fcoll = lambda batch: tuple(zip(*batch))\n",
    "\n",
    "dataset = SubCocoDataset(datadir/froot/'train', stats, transform=tfm)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1, collate_fn=fcoll)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "(images), (targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wrap the Dataset into a DataModule..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class SubCocoDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, root, stats, bs=32, workers=4, split_ratio=0.8):\n",
    "        super().__init__()\n",
    "        self.dir = root\n",
    "        self.bs = bs\n",
    "        self.workers = workers \n",
    "        self.stats = stats\n",
    "        self.split_ratio = split_ratio\n",
    "        \n",
    "        # transforms for images\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(stats.chn_means, stats.chn_stds)\n",
    "        ])\n",
    "\n",
    "        # prepare transforms for coco object detection\n",
    "        dataset = SubCocoDataset(self.dir, self.stats, transform=transform)\n",
    "        num_items = len(dataset)\n",
    "        num_train = int(self.split_ratio*num_items)\n",
    "        self.train, self.val = random_split(dataset, (num_train, num_items-num_train), generator=torch.Generator().manual_seed(42))\n",
    "        print(self.train, self.val)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.bs, num_workers=self.workers, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.bs, num_workers=self.workers, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "tiny_coco_dm = SubCocoDataModule(datadir/froot/'train', stats, bs=2)\n",
    "tdl=tiny_coco_dm.train_dataloader()\n",
    "images, targets = next(iter(tdl))\n",
    "len(images), len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture Model\n",
    "\n",
    "There are Many object detection models to choose from.\n",
    "\n",
    "To break my anaylysis paralysis researching and deciding between the various models and architectures, I decided to push ahead with what is provided by TorchVision out of the box i.e. Faster R-CNN with pretrained ResNet backbone. \n",
    "\n",
    "Luckily there is a turorial to follow\n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just use the raw model directly and verify the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, stats.num_cats)\n",
    "\n",
    "tdl = tiny_coco_dm.train_dataloader()\n",
    "images, targets = next(iter(tdl))\n",
    "images = list(img for img in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "# how to get both in 1 step?\n",
    "losses = model(images,targets) \n",
    "\n",
    "model.eval()\n",
    "predict = model(images)\n",
    "(predict, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In my initial attempt to port the example code Faster RCNN code to Pytorch-Lightning, I realized soon computing metrics is going to tricky. In fact, I don't actually know even what metrics to use!  Luckily someone pointed me to this well written [article by Raphael Padilla](https://github.com/rafaelpadilla/Object-Detection-Metrics) which explains the metrics used in object detection today. \n",
    "\n",
    "I decided to repurpose [CocoAPI](https://github.com/cocodataset/cocoapi/) which has builtin metric evaluation instead of rolling my own evaluation metrics.\n",
    "\n",
    "It was painful as Coco API was not written for easy extension in mind.  For example, it tightly couples JSON file loading (and its format) into metrics computation, so I had to look under the cover for its implementation to then use it in a way that is probably not appropriate.  Also it tighly couple the computation of metrics with the batching of data.\n",
    "\n",
    "I ended up just writing a wrapper that uses a separate Coco object for each ground truth and prediction, as if the whole epoch has only 1 sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SubCocoWrapper():\n",
    "    def __init__(self, categories, p, t):\n",
    "        # turn tgt: { \"boxes\": [...], \"labels\": [...], \"image_id\": \"xxx\", \"area\": [...], \"iscrowd\": 0 }\n",
    "        # into COCO with dataset dict of this form:\n",
    "        # { images: [], categories: [], annotations: [{\"image_id\": int, \"category_id\": int, \"bbox\": (x,y,width,height)}, ...] }\n",
    "        # see https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py\n",
    "        with io.capture_output() as captured:\n",
    "            self.target = COCO()\n",
    "            img_id = int(t[\"image_id\"]) # could be tensor, cast to int\n",
    "            images = [ {'id': img_id, 'file_name': f\"{img_id:012d}.jpg\"} ]\n",
    "            self.target.dataset[\"images\"] = images\n",
    "            self.target.dataset[\"categories\"] = categories\n",
    "            self.target.dataset[\"annotations\"] = []\n",
    "            for bi, b in enumerate(t[\"boxes\"]):\n",
    "                x, y, w, h = b\n",
    "                cat_id = t[\"labels\"][bi]\n",
    "                anno_id = t[\"ids\"][bi]\n",
    "                self.target.dataset[\"annotations\"].append({'id': anno_id, 'image_id': img_id, 'category_id': cat_id, 'bbox': b})\n",
    "            self.target.createIndex()\n",
    "\n",
    "            # [ {'boxes': tensor([[100.5,  39.7, 109.1,  52.7], [110.9,  41.1, 120.4,  54.4], [ 36.6,  56.1,  46.9,  74.0]], device='cuda:0'), \n",
    "            #    'labels': tensor([1, 1, 1], device='cuda:0'), \n",
    "            #    'scores': tensor([0.7800, 0.7725, 0.7648], device='cuda:0')}, ...]\n",
    "            # numpy array [Nx7] of {imageID,x1,y1,w,h,score,class}\n",
    "            pna = np.zeros((len(p[\"boxes\"]), 7))\n",
    "            for bi, b in enumerate(p[\"boxes\"]):\n",
    "                pna[bi]=(img_id, *b, p[\"scores\"][bi], p[\"labels\"][bi])\n",
    "\n",
    "            anns = self.target.loadNumpyAnnotations(pna)\n",
    "            self.prediction = COCO()\n",
    "            self.prediction.dataset[\"images\"] = images\n",
    "            self.prediction.dataset[\"categories\"] = categories\n",
    "            self.prediction.dataset[\"annotations\"] = anns\n",
    "\n",
    "    def targetCoco(self): \n",
    "        return self.target\n",
    "    \n",
    "    def predictionCoco(self): \n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above wrapper, I can then compute the metric for the 1 sample epoch to get the metrics I want for the image\n",
    "* Mean Average Precision (MAP)\n",
    "* Mean Average Recall (MAR) \n",
    "* over a range of Intersection over Union (IOU) values from 50% to 95%\n",
    "* then combined it using [F1](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FRCNN(LightningModule):\n",
    "    def __init__(self, lbl2cat):\n",
    "        super(FRCNN, self).__init__()\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.categories = [ {'id': lid, 'name': f\"{cid}\" } for lid, cid in lbl2cat.items() ]\n",
    "        self.num_classes = len(self.categories)  \n",
    "        \n",
    "        # get number of input features for the classifier\n",
    "        self.in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(self.in_features, self.num_classes)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        losses = self.model(x, y)\n",
    "        loss = sum(losses.values())\n",
    "        logs = {'train_loss': loss} \n",
    "        return {'loss': loss, 'log': logs} # should add 'acc' accuracy e.g. MAP, MAR etc\n",
    "\n",
    "    def metrics(self, preds, targets):\n",
    "        accu = torch.zeros((len(preds), 1))\n",
    "        for i, (p,t) in enumerate(zip(preds, targets)):\n",
    "            subcoco = SubCocoWrapper(self.categories, p, t)\n",
    "            cocoeval = COCOeval(subcoco.targetCoco(), subcoco.predictionCoco(), \"bbox\")\n",
    "            with io.capture_output() as captured:\n",
    "                cocoeval.evaluate()\n",
    "                cocoeval.accumulate()\n",
    "                cocoeval.summarize()\n",
    "            precision = cocoeval.stats[0] # Average Precision (AP) @[ IoU=0.50:0.95 | area=all | maxDets=100 ]\n",
    "            recall = cocoeval.stats[8] # Average Recall (AR) @[ IoU=0.50:0.95 | area=all | maxDets=100 ]\n",
    "            f1 = 2/(1/precision + 1/recall) # see https://en.wikipedia.org/wiki/F1_score\n",
    "            accu[i] = f1\n",
    "        return accu\n",
    "        \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        # validation runs the model in eval mode, so Y is prediction, not losses\n",
    "        xs, ys = val_batch\n",
    "        preds = self.model(xs, ys)\n",
    "        accu = self.metrics(preds, ys)\n",
    "        return {'val_acc': accu} # should add 'val_acc' accuracy e.g. MAP, MAR etc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # called at the end of the validation epoch\n",
    "        # outputs is an array with what you returned in validation_step for each batch\n",
    "        # outputs = [{'loss': batch_0_loss}, {'loss': batch_1_loss}, ..., {'loss': batch_n_loss}] \n",
    "        avg_loss = torch.stack([ o['val_acc'] for o in outputs ]).mean()\n",
    "        tensorboard_logs = {'val_acc': avg_loss}\n",
    "        return {'avg_val_acc': avg_loss, 'val_acc': tensorboard_logs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can train a few epocs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frcnn_model = FRCNN(stats.lbl2cat)\n",
    "tiny_coco_dm = SubCocoDataModule(datadir/froot/'train', stats, bs=2) # on my small GPU W/ 4GB VRAM, I can only fit bs=2\n",
    "chkpt_cb = ModelCheckpoint(\n",
    "    filepath='model/tiny-coco.ckpt',\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "# train\n",
    "trainer = Trainer(gpus=1, max_epochs=20, checkpoint_callback=chkpt_cb, accumulate_grad_batches=8)\n",
    "trainer.fit(frcnn_model, tiny_coco_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/ --host \"0.0.0.0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference\n",
    "\n",
    "Time to see how well the model can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model([img[0]])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digest_pred(l2c, pred, cutoff=0.4):\n",
    "    scores = pred['scores']\n",
    "    pass_idxs = (scores > cutoff).nonzero(as_tuple=False)\n",
    "    lbls = pred['labels'][pass_idxs]\n",
    "    bboxs = pred['boxes'][pass_idxs]\n",
    "    c2ibs = defaultdict(lambda: [])\n",
    "    for i,lb in enumerate(zip(lbls, bboxs)):\n",
    "        l,b = lb\n",
    "        x,y,w,h = b[0]\n",
    "        c = l2c[l.item()]\n",
    "        ibs = c2ibs[c]\n",
    "        ibs.append((i,x.item(),y.item(),w.item(),h.item()))\n",
    "        c2ibs[c] = ibs\n",
    "    return c2ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c2ibs = digest_pred(stats.lbl2cat, pred[0])\n",
    "pimg = torchvision.transforms.ToPILImage()(img[0])\n",
    "\n",
    "overlay_img_bbox(pimg, pred_c2ibs, stats.cat2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstitude from Saved Model\n",
    "Now let's reload from checkpoint to see if all works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = FRCNN.load_from_checkpoint(\"model/*.ckpt\")\n",
    "pretrained_model.freeze()\n",
    "pred = pretrained_model([img[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
