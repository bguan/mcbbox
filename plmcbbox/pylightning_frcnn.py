# AUTOGENERATED! DO NOT EDIT! File to edit: 00_pylightning_frcnn.ipynb (unless otherwise specified).

__all__ = [
    'fetch_data', 'froot', 'fname', 'url', 'datadir', 'json_fname',
    'CocoDatasetStats', 'stats', 'bbox_to_rect', 'label_for_bbox',
    'overlay_img_bbox', 'SubCocoDataset', 'CocoDataModule', 'tiny_coco_dm',
    'tdl', 'model', 'in_features', 'tdl', 'images', 'targets', 'output',
    'FRCNN'
]

# Cell
import json, os, requests, sys, tarfile, torch, torchvision
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import torch.nn.functional as F
import pytorch_lightning as pl

from collections import defaultdict
from pathlib import Path
from PIL import Image

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import *

from torch import nn
from torch import optim
from torch.utils.data import DataLoader, random_split

from torchvision import transforms
from torchvision.datasets import CocoDetection
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

from tqdm import tqdm

# Cell


def fetch_data(url: str,
               datadir: Path,
               tgt_fname: str,
               chunk_size: int = 8 * 1024):
    dest = datadir / tgt_fname
    print(f"Downloading from {url} to {dest}...")
    with requests.get(url, stream=True, timeout=10) as response:
        content_len = int(response.headers['content-length'])
        with open(dest, 'wb') as f:
            with tqdm(total=content_len) as pbar:
                nbytes = 0
                num_chunks = 0
                for chunk in response.iter_content(chunk_size=chunk_size):
                    chunk_len = len(chunk)
                    nbytes += chunk_len
                    num_chunks += 1
                    f.write(chunk)
                    pbar.update(chunk_len)

    with tarfile.open(dest, 'r') as tar:
        extracted = []
        for item in tar:
            tar.extract(item, datadir)
            extracted.append(item.name)

    print(
        f"Downloaded {nbytes} from {url} to {dest}, extracted in {datadir}: {extracted[:3]},...,{extracted[-3:]}"
    )


# Cell

froot = "coco_tiny"
fname = f"{froot}.tgz"
url = f"http://files.fast.ai/data/examples/{fname}"
datadir = Path("workspace")
fetch_data(url, datadir, fname, chunk_size=1024)

# Cell

json_fname = datadir / froot / 'train.json'
with open(json_fname, 'r') as json_f:
    ann = json.load(json_f)


# Cell
class CocoDatasetStats():
    # num_cats
    # num_imgs
    # num_bboxs
    # cat2name
    # img2fname
    # imgs
    # img2cat2bs
    # cat2ibs
    # avg_ncats_per_img
    # avg_nboxs_per_img
    # avg_nboxs_per_cat
    # chn_means
    # chn_stds
    def __init__(self, ann: dict):
        self.num_cats = len(ann['categories'])
        self.num_imgs = len(ann['images'])
        self.num_bboxs = len(ann['annotations'])

        # build cat id to name and img_id to file map
        self.cat2name = {c['id']: c['name'] for c in ann['categories']}
        self.img2fname = {img['id']: img['file_name'] for img in ann['images']}
        self.imgs = [{
            'id': img_id,
            'file_name': img_fname
        } for (img_id, img_fname) in self.img2fname.items()]

        # build up 2 maps for later analysis
        self.img2cat2ibs = defaultdict(lambda: defaultdict(lambda: set()))
        self.img2liibs = defaultdict(lambda: [])
        self.cat2iibs = defaultdict(lambda: set())
        anno_id = 0
        for a in ann['annotations']:
            img_id = a['image_id']
            cat_id = a['category_id']
            c2ibs_for_img = self.img2cat2ibs[img_id]
            (x, y, w, h) = a['bbox']
            ib = (anno_id, x, y, w, h)
            iib = (img_id, *ib)
            liib = (cat_id, *iib)
            anno_id += 1
            c2ibs_for_img[cat_id].add(ib)
            self.cat2iibs[cat_id].add(iib)
            self.img2cat2ibs[img_id] = c2ibs_for_img
            self.img2liibs[img_id].append(liib)

        acc_ncats_per_img = 0.0
        acc_nboxs_per_img = 0.0
        for img_id, c2ibs in self.img2cat2ibs.items():
            acc_ncats_per_img += len(c2ibs)
            for cat_id, ibs in c2ibs.items():
                acc_nboxs_per_img += len(ibs)

        self.avg_ncats_per_img = acc_ncats_per_img / self.num_imgs
        self.avg_nboxs_per_img = acc_nboxs_per_img / self.num_imgs

        acc_nboxs_per_cat = 0.0
        for cat_id, iibs in self.cat2iibs.items():
            acc_nboxs_per_cat += len(iibs)

        self.avg_nboxs_per_cat = acc_nboxs_per_cat / self.num_cats

        # compute Images per channel means and std deviation using Welfordâ€™s method

        n = 0
        mean = np.zeros((3, ))
        M2 = np.zeros((3, ))

        for img in self.imgs:
            fname = img['file_name']
            n = n + 1
            img = Image.open(datadir / froot / 'train' / fname)
            ia = np.asarray(img)
            x = np.mean(ia, axis=(0, 1))
            delta = x - mean
            mean = mean + delta / n
            M2 = M2 + delta * (x - mean)

        variance = M2 / (n - 1)

        self.chn_means = mean
        self.chn_stds = np.sqrt(variance)


# Cell

stats = CocoDatasetStats(ann)

print(
    f"Categories {stats.num_cats}, Images {stats.num_imgs}, Boxes {stats.num_bboxs}, "
    f"avg cats/img {stats.avg_ncats_per_img:.1f}, avg boxs/img {stats.avg_nboxs_per_img:.1f}, avg boxs/cat {stats.avg_nboxs_per_cat:.1f}."
)

print(
    f"Image means by channel {stats.chn_means}, std.dev by channel {stats.chn_stds}"
)

# Cell


def bbox_to_rect(ibbox, color):
    return plt.Rectangle(xy=(ibbox[1], ibbox[2]),
                         width=ibbox[3],
                         height=ibbox[4],
                         fill=False,
                         edgecolor=color,
                         linewidth=2)


def label_for_bbox(ibbox, label, color):
    return plt.text(ibbox[1],
                    ibbox[2],
                    f"{ibbox[0]}.{label}",
                    color=color,
                    fontsize=16)


def overlay_img_bbox(img_fname: str, cat2ibs: dict, cat2name: dict):
    cat2color = {
        cid: cname
        for (cid, cname) in zip(cat2ibs.keys(), mcolors.TABLEAU_COLORS.keys())
    }
    img = Image.open(img_fname)
    fig = plt.figure(figsize=(16, 10))
    fig = plt.imshow(img)
    for cid, ibs in cat2ibs.items():
        for ib in ibs:
            label_for_bbox(ib, cat2name[cid], cat2color[cid])
            fig.axes.add_patch(bbox_to_rect(ib, cat2color[cid]))


# Cell


class SubCocoDataset(torchvision.datasets.VisionDataset):
    """
    Simulate what torchvision.CocoDetect() returns for target given fastai's coco subsets
    Args:
        root (string): Root directory where images are downloaded to.
        stats (CocoDatasetStats):
        transform (callable, optional): A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.ToTensor``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
        transforms (callable, optional): A function/transform that takes input sample and its target as entry
            and returns a transformed version.
    """
    def __init__(self,
                 root,
                 stats,
                 transform=None,
                 target_transform=None,
                 transforms=None):
        super(SubCocoDataset, self).__init__(root, transforms, transform,
                                             target_transform)
        self.stats = stats
        self.img_ids = list(stats.img2fname.keys())

    def __getitem__(self, index):
        """
        Args:
            index (int): Index
        Returns:
            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.
        """
        img_id = self.img_ids[index] if index < len(self.img_ids) else 0
        img_fname = self.stats.img2fname.get(img_id, None)
        if img_id == None or img_fname == None:
            print(
                f"__getitem__({index}): got img_id {img_id}, img_fname {img_fname}"
            )
            return (None, None)

        img = Image.open(os.path.join(self.root, img_fname)).convert('RGB')
        target = {
            "boxes": [],
            "labels": [],
            "image_id": None,
            "area": [],
            "iscrowd": 0
        }
        count = 0
        liibs = self.stats.img2liibs.get(img_id, [])
        for cat_id, img_id, anno_id, x, y, w, h in liibs:
            count += 1
            target["boxes"].append([x, y, x + w, y + h])
            target["labels"].append(cat_id)
            target["image_id"] = img_id
            target["area"].append(w * h)

        for k, v in target.items():
            target[k] = torch.tensor(v)

        if self.transforms is not None:
            img, target = self.transforms(img, target)
        else:
            if self.transform is not None: img = self.transform(img)
            if self.target_transform is not None:
                target = self.target_transform(target)

        return img, target

    def __len__(self):
        return self.stats.num_imgs


# Cell


class CocoDataModule(LightningDataModule):
    def __init__(self, root, stats, bs=32, workers=0, split_ratio=0.8):
        super().__init__()
        self.dir = root
        self.bs = bs
        self.workers = workers
        self.stats = stats
        self.split_ratio = split_ratio

        # transforms for images
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(stats.chn_means, stats.chn_stds)
        ])

        # prepare transforms for coco object detection
        dataset = SubCocoDataset(self.dir, self.stats, transform=transform)
        num_items = len(dataset)
        num_train = int(self.split_ratio * num_items)
        self.train, self.val = random_split(
            dataset, (num_train, num_items - num_train),
            generator=torch.Generator().manual_seed(42))
        print(self.train, self.val)

    def collate_fn(self, batch):
        return tuple(zip(*batch))

    def train_dataloader(self):
        return DataLoader(self.train,
                          batch_size=self.bs,
                          num_workers=self.workers,
                          collate_fn=self.collate_fn)

    def val_dataloader(self):
        return DataLoader(self.val,
                          batch_size=self.bs,
                          num_workers=self.workers,
                          collate_fn=self.collate_fn)


# Cell

tiny_coco_dm = CocoDataModule(datadir / froot / 'train', stats, bs=2)
tdl = tiny_coco_dm.train_dataloader()
images, targets = next(iter(tdl))
len(images), len(targets)

# Cell

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, stats.num_cats)

tdl = tiny_coco_dm.train_dataloader()
images, targets = next(iter(tdl))
images = list(img for img in images)
targets = [{k: v for k, v in t.items()} for t in targets]

output = model(images, targets)

# Cell


class FRCNN(LightningModule):
    def __init__(self, num_classes):
        super(FRCNN, self).__init__()
        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(
            pretrained=True)
        self.num_classes = num_classes

        # get number of input features for the classifier
        self.in_features = self.model.roi_heads.box_predictor.cls_score.in_features
        # replace the pre-trained head with a new one
        self.model.roi_heads.box_predictor = FastRCNNPredictor(
            self.in_features, self.num_classes)

    def forward(self, x):
        batch_size, channels, width, height = x.size()

        # (b, 3, w, h) -> (b, 1*w*h)
        x = x.view(batch_size, -1)
        y = self.model(x)
        return x

    def cross_entropy_loss(self, logits, labels):
        return F.nll_loss(logits, labels)

    def training_step(self, train_batch, batch_idx):
        x, y = train_batch
        logits = self.forward(x)
        loss = self.cross_entropy_loss(logits, y)
        logs = {'train_loss': loss}
        return {'loss': loss, 'log': logs}

    def validation_step(self, val_batch, batch_idx):
        x, y = val_batch
        logits = self.forward(x)
        loss = self.cross_entropy_loss(logits, y)
        return {'val_loss': loss}

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

    def validation_epoch_end(self, outputs):
        # called at the end of the validation epoch
        # outputs is an array with what you returned in validation_step for each batch
        # outputs = [{'loss': batch_0_loss}, {'loss': batch_1_loss}, ..., {'loss': batch_n_loss}]
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        tensorboard_logs = {'val_loss': avg_loss}
        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
