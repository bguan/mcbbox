# AUTOGENERATED! DO NOT EDIT! File to edit: 00_pylightning_frcnn.ipynb (unless otherwise specified).

__all__ = ['fetch_data', 'empty_list', 'empty_set', 'cat_2_empty_set', 'CocoDatasetStats', 'bbox_to_rect',
           'label_for_bbox', 'overlay_img_bbox', 'SubCocoDataset', 'SubCocoDataModule', 'SubCocoWrapper', 'iou_calc',
           'accuracy_1img', 'FRCNN', 'digest_pred']

# Cell
import json, os, requests, sys, tarfile, torch, torchvision
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import pickle
import pytorch_lightning as pl
import torch.nn.functional as F

from collections import defaultdict
from IPython.utils import io
from pathlib import Path
from PIL import Image
from PIL import ImageStat

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import *

from torch import nn
from torch import optim
from torch.utils.data import DataLoader, random_split

from torchvision import transforms
from torchvision.datasets import CocoDetection
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

from tqdm import tqdm

# Cell

def fetch_data(url:str, datadir: Path, tgt_fname:str, chunk_size:int=8*1024, quiet=True):
    dest = datadir/tgt_fname
    if not quiet: print(f"Downloading from {url} to {dest}...")
    with requests.get(url, stream=True, timeout=10) as response:
        content_len = int(response.headers['content-length'])
        with open(dest, 'wb') as f:
            with tqdm(total=content_len) as pbar:
                nbytes = 0
                num_chunks = 0
                for chunk in response.iter_content(chunk_size=chunk_size):
                    chunk_len = len(chunk)
                    nbytes += chunk_len
                    num_chunks += 1
                    f.write(chunk)
                    pbar.update(chunk_len)

    with tarfile.open(dest, 'r') as tar:
        extracted = []
        for item in tar:
            tar.extract(item, datadir)
            extracted.append(item.name)

    if not quiet: print(f"Downloaded {nbytes} from {url} to {dest}, extracted in {datadir}: {extracted[:3]},...,{extracted[-3:]}")

# Cell

# cannot use lambda as pickling may fail when saving models
def empty_list(): return []
def empty_set(): return set()
def cat_2_empty_set(): return defaultdict(empty_set)

class CocoDatasetStats():
    # num_cats
    # num_imgs
    # num_bboxs
    # cat2name
    # lbl2cat
    # cat2lbl
    # img2fname
    # imgs
    # img2cat2bs
    # cat2ibs
    # avg_ncats_per_img
    # avg_nboxs_per_img
    # avg_nboxs_per_cat
    # chn_means
    # chn_stds
    def __init__(self, ann:dict, img_dir:Path):

        self.img_dir = img_dir
        self.num_cats = len(ann['categories'])
        self.num_imgs = len(ann['images'])
        self.num_bboxs = len(ann['annotations'])

        # build cat id to name, assign FRCNN
        self.cat2name = { c['id']: c['name'] for c in ann['categories'] }

        # expected labels w 0 = background
        self.lbl2cat = { l+1: c for l, (c, n) in enumerate(self.cat2name.items()) }
        self.cat2lbl = { c: l+1 for l, (c, n) in enumerate(self.cat2name.items()) }
        self.lbl2cat[0] = (0, 'background')
        self.cat2lbl[0] = 0

        # img_id to file map
        self.img2fname = { img['id']:img['file_name'] for img in ann['images'] }
        self.imgs = [ { 'id':img_id, 'file_name':img_fname } for (img_id, img_fname) in self.img2fname.items() ]

        # build up some maps for later analysis
        self.img2cat2ibs = defaultdict(cat_2_empty_set)
        self.img2liibs = defaultdict(empty_list)
        self.cat2iibs = defaultdict(empty_set)
        anno_id = 0
        for a in ann['annotations']:
            img_id = a['image_id']
            cat_id = a['category_id']
            c2ibs_for_img = self.img2cat2ibs[img_id]
            (x, y, w, h) = a['bbox']
            ib = (anno_id, x, y, w, h)
            iib = (img_id, *ib)
            liib = (cat_id, *iib)
            c2ibs_for_img[cat_id].add(ib)
            self.cat2iibs[cat_id].add(iib)
            self.img2cat2ibs[img_id] = c2ibs_for_img
            self.img2liibs[img_id].append(liib)
            anno_id +=1

        acc_ncats_per_img = 0.0
        acc_nboxs_per_img = 0.0
        for img_id, c2ibs in self.img2cat2ibs.items():
            acc_ncats_per_img += len(c2ibs)
            for cat_id, ibs in c2ibs.items():
                acc_nboxs_per_img += len(ibs)

        self.avg_ncats_per_img = acc_ncats_per_img/self.num_imgs
        self.avg_nboxs_per_img = acc_nboxs_per_img/self.num_imgs

        acc_nboxs_per_cat = 0.0
        for cat_id, iibs in self.cat2iibs.items():
            acc_nboxs_per_cat += len(iibs)

        self.avg_nboxs_per_cat = acc_nboxs_per_cat/self.num_cats

        # compute Images per channel means and std deviation using PIL.ImageStat.Stat()

        n = 0
        mean = np.zeros((3,))
        stddev = np.zeros((3,))

        for img in tqdm(self.imgs):
            fname = img['file_name']
            n = n + 1
            istat = ImageStat.Stat(Image.open(img_dir/fname))
            mean = (istat.mean + (n-1)*mean)/n
            stddev = (istat.stddev + (n-1)*stddev)/n

        self.chn_means = mean
        self.chn_stds = stddev

# Cell

def bbox_to_rect(ibbox, color):
    return plt.Rectangle(
        xy=(ibbox[1], ibbox[2]), width=ibbox[3], height=ibbox[4],
        fill=False, edgecolor=color, linewidth=2)

def label_for_bbox(ibbox, label, color):
    return plt.text(ibbox[1], ibbox[2], f"{ibbox[0]}.{label}", color=color, fontsize=16)

def overlay_img_bbox(img:Image, cat2ibs: dict, cat2name: dict):
    cat2color = { cid: cname for (cid, cname) in zip(cat2ibs.keys(), mcolors.TABLEAU_COLORS.keys()) }
    fig = plt.figure(figsize=(16,10))
    fig = plt.imshow(img)
    for cid, ibs in cat2ibs.items():
        for ib in ibs:
            label_for_bbox(ib, cat2name[cid], cat2color[cid])
            fig.axes.add_patch(bbox_to_rect(ib, cat2color[cid]))

# Cell

class SubCocoDataset(torchvision.datasets.VisionDataset):
    """
    Simulate what torchvision.CocoDetect() returns for target given fastai's coco subsets
    Args:
        root (string): Root directory where images are downloaded to.
        stats (CocoDatasetStats):
        transform (callable, optional): A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.ToTensor``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
        transforms (callable, optional): A function/transform that takes input sample and its target as entry
            and returns a transformed version.
    """

    def __init__(self, root, stats, transform=None, target_transform=None, transforms=None):
        super(SubCocoDataset, self).__init__(root, transforms, transform, target_transform)
        self.stats = stats
        self.img_ids = list(stats.img2fname.keys())

    def __getitem__(self, index):
        """
        Args:
            index (int): Index
        Returns:
            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.
        """
        img_id = self.img_ids[index] if index < len(self.img_ids) else 0
        img_fname = self.stats.img2fname.get(img_id, None)
        if img_id == None or img_fname ==None:
            return (None, None)

        img = Image.open(os.path.join(self.root, img_fname)).convert('RGB')
        target = { "boxes": [], "labels": [], "image_id": None, "area": [], "iscrowd": 0, "ids": [] }
        count = 0
        liibs = self.stats.img2liibs.get(img_id,[])
        for cat_id, img_id, anno_id, x, y, w, h in liibs:
            count += 1
            target["boxes"].append([x, y, x+w, y+h])
            target["labels"].append(self.stats.cat2lbl[cat_id])
            target["image_id"] = img_id
            target["area"].append(w*h)
            target["ids"].append(anno_id)

        for k, v in target.items():
            target[k] = torch.tensor(v)

        if self.transforms is not None:
            img, target = self.transforms(img, target)
        else:
            if self.transform is not None: img = self.transform(img)
            if self.target_transform is not None: target = self.target_transform(target)

        return img, target

    def __len__(self):
        return self.stats.num_imgs

# Cell

class SubCocoDataModule(LightningDataModule):

    def __init__(self, root, stats, bs=32, workers=4, split_ratio=0.8):
        super().__init__()
        self.dir = root
        self.bs = bs
        self.workers = workers
        self.stats = stats
        self.split_ratio = split_ratio

        # transforms for images
        transform=transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(stats.chn_means, stats.chn_stds)
        ])

        # prepare transforms for coco object detection
        dataset = SubCocoDataset(self.dir, self.stats, transform=transform)
        num_items = len(dataset)
        num_train = int(self.split_ratio*num_items)
        self.train, self.val = random_split(dataset, (num_train, num_items-num_train), generator=torch.Generator().manual_seed(42))
        # print(self.train, self.val)

    def collate_fn(self, batch):
        return tuple(zip(*batch))

    def train_dataloader(self):
        return DataLoader(self.train, batch_size=self.bs, num_workers=self.workers, collate_fn=self.collate_fn)

    def val_dataloader(self):
        return DataLoader(self.val, batch_size=self.bs, num_workers=self.workers, collate_fn=self.collate_fn)

# Cell

class SubCocoWrapper():
    def __init__(self, categories, p, t):
        # turn tgt: { "boxes": [...], "labels": [...], "image_id": "xxx", "area": [...], "iscrowd": 0 }
        # into COCO with dataset dict of this form:
        # { images: [], categories: [], annotations: [{"image_id": int, "category_id": int, "bbox": (x,y,width,height)}, ...] }
        # see https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py
        with io.capture_output() as captured:
            self.target = COCO()
            img_id = int(t["image_id"]) # could be tensor, cast to int
            images = [ {'id': img_id, 'file_name': f"{img_id:012d}.jpg"} ]
            self.target.dataset["images"] = images
            self.target.dataset["categories"] = categories
            self.target.dataset["annotations"] = []
            for bi, b in enumerate(t["boxes"]):
                x, y, w, h = b
                cat_id = t["labels"][bi]
                anno_id = t["ids"][bi]
                self.target.dataset["annotations"].append({'id': anno_id, 'image_id': img_id, 'category_id': cat_id, 'bbox': b})
            self.target.createIndex()

            # [ {'boxes': tensor([[100.5,  39.7, 109.1,  52.7], [110.9,  41.1, 120.4,  54.4], [ 36.6,  56.1,  46.9,  74.0]], device='cuda:0'),
            #    'labels': tensor([1, 1, 1], device='cuda:0'),
            #    'scores': tensor([0.7800, 0.7725, 0.7648], device='cuda:0')}, ...]
            # numpy array [Nx7] of {imageID,x1,y1,w,h,score,class}
            pna = np.zeros((len(p["boxes"]), 7))
            for bi, b in enumerate(p["boxes"]):
                pna[bi]=(img_id, *b, p["scores"][bi], p["labels"][bi])

            anns = self.target.loadNumpyAnnotations(pna)
            self.prediction = COCO()
            self.prediction.dataset["images"] = images
            self.prediction.dataset["categories"] = categories
            self.prediction.dataset["annotations"] = anns

    def targetCoco(self):
        return self.target

    def predictionCoco(self):
        return self.prediction

# Cell
def iou_calc(x1,y1,w1,h1, x2,y2,w2,h2):
    r1 = x1+w1 # right of box1
    b1 = y1+h1 # bottom of box1
    r2 = x2+w2 # right of box2
    b2 = y2+h2 # bottom of box2
    a1 = w1*h1
    a2 = w2*h2
    ia = 0. # intercept
    if x1 <= x2 <= r1:
        if y1 <= y2 <= b1:
            ia = (r1-x2)*(b1-y2)
        elif y1 <= b2 <= b1:
            ia = (r1-x2)*(b1-b2)
    elif x1 <= r2 <= r1:
        if y1 <= y2 <= b1:
            ia = (r1-r2)*(b1-y2)
        elif y1 <= b2 <= b1:
            ia = (r1-r2)*(b1-b2)
    iou = ia/(a1+a2-ia)
    # print(x1,y1,w1,h1, x2,y2,w2,h2,iou)
    return iou

def accuracy_1img(pred, tgt, scut=0.5, ithr=0.5):
    scut = 0.6
    ithr = 0.1
    pscores = pred['scores']
    pidxs = (pscores > scut).nonzero(as_tuple=True)
    pboxs = pred['boxes'][pidxs]
    tboxs = tgt['boxes']
    tls = tgt['labels']
    pls = pred['labels']
    tlset = {l for l in tls}

    tl2num = defaultdict(lambda:0)
    for tl in tls: tl2num[tl]+=1

    tlpls = []
    for tl,tb in zip(tls,tboxs):
        for pl,pb in zip(pls,pboxs):
            iou = iou_calc(*tb, *pb)
            # print(iou)
            if iou > ithr: tlpls.append((tl,pl))

    tl2tpfp = defaultdict(lambda: (0,0))
    for tl in tlset:
        for tl, pl in tlpls:
            tp, fp = tl2tpfp[tl]
            if tl == pl:
                tp+=1
            else:
                fp+=1
            tl2tpfp[tl] = (tp, fp)

    tl2f1 = {}
    for tl in tlset:
        tp, fp = tl2tpfp[tl]
        tlnum = tl2num[tl]
        precision = 0 if tp == 0 else tp/(tp+fp)
        recall = 0 if tp == 0 else tp/tlnum
        f1 = 0 if precision*recall == 0 else 2/(1/precision + 1/recall)
        tl2f1[tl] = f1

    acc = 0.
    tnum = len(tgt['boxes'])
    for tl, f1 in tl2f1.items():
        tlnum = tl2num[tl]
        f1 = tl2f1[tl]
        acc += f1*(tlnum/tnum)

    return acc

# Cell

class FRCNN(LightningModule):
    def __init__(self, lbl2cat):
        super(FRCNN, self).__init__()
        self.categories = [ {'id': lid, 'name': f"{cid}" } for lid, cid in lbl2cat.items() ]
        self.num_classes = len(self.categories)

        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

        # lock the pretrained model body
        for param in self.model.parameters():
            param.requires_grad = False

        # get number of input features for the classifier
        self.in_features = self.model.roi_heads.box_predictor.cls_score.in_features

        # replace the pre-trained head with a new one, which is trainable
        self.model.roi_heads.box_predictor = FastRCNNPredictor(self.in_features, self.num_classes)

    def training_step(self, train_batch, batch_idx):
        x, y = train_batch
        losses = self.model(x, y)
        loss = sum(losses.values())
        result = TrainResult(loss)
        result.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return result

    def metrics(self, preds, targets):
        accu = torch.zeros((len(preds), 1))
        for i, (p,t) in enumerate(zip(preds, targets)):
            accu[i] = accuracy_1img(p, t, .3, .3)
        return torch.tensor(accu)

    def validation_step(self, val_batch, batch_idx):
        # validation runs the model in eval mode, so Y is prediction, not losses
        xs, ys = val_batch
        preds = self.model(xs, ys)
        accu = self.metrics(preds, ys)
        return {'val_acc': accu} # should add 'val_acc' accuracy e.g. MAP, MAR etc

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

    def validation_epoch_end(self, outputs):
        # called at the end of the validation epoch, but gradient accumulation may result in last row being different size
        val_accs = np.concatenate([ (o['val_acc']).numpy() for o in outputs ])
        avg_acc = val_accs.mean()
        tensorboard_logs = {'val_acc': avg_acc}
        return {'val_acc': avg_acc, 'logs': tensorboard_logs}

    def forward(self, x):
        self.model.eval()
        pred = self.model(x)
        return pred

# Cell
def digest_pred(l2c, pred, cutoff=0.5):
    scores = pred['scores']
    pass_idxs = (scores > cutoff).nonzero(as_tuple=False)
    lbls = pred['labels'][pass_idxs]
    bboxs = pred['boxes'][pass_idxs]
    c2ibs = defaultdict(lambda: [])
    for i,lb in enumerate(zip(lbls, bboxs)):
        l,b = lb
        x,y,w,h = b[0]
        c = l2c[l.item()]
        ibs = c2ibs[c]
        ibs.append((i,x.item(),y.item(),w.item(),h.item()))
        c2ibs[c] = ibs
    return c2ibs