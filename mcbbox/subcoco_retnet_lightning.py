# AUTOGENERATED! DO NOT EDIT! File to edit: 40_subcoco_retinanet_lightning.ipynb (unless otherwise specified).

__all__ = ['RetinaNetModule', 'run_training', 'save_final']

# Cell
import cv2, json, os, requests, sys, tarfile, torch, torchvision
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import pickle
import random
import torch.nn.functional as F
import torch.multiprocessing

from collections import defaultdict
from functools import reduce
from IPython.utils import io
from pathlib import Path
from PIL import Image
from PIL import ImageStat

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

from tqdm import tqdm
from typing import Hashable, List, Tuple, Union


# Cell
import albumentations as A
import pytorch_lightning as pl
from albumentations.pytorch import ToTensorV2
from gpumonitor.monitor import GPUStatMonitor
from gpumonitor.callbacks.lightning import PyTorchGpuMonitorCallback
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning import LightningDataModule, LightningModule, Trainer
from pytorch_lightning.core.step_result import TrainResult
from torch import nn
from torch import optim
from torch.utils.data import DataLoader, random_split
from torchvision.models.detection import RetinaNet, retinanet_resnet50_fpn
from torchvision import transforms
from .subcoco_utils import *
from .subcoco_lightning_utils import *

torch.multiprocessing.set_sharing_strategy('file_system')
print(f"Python ver {sys.version}, torch {torch.__version__}, torchvision {torchvision.__version__}, pytorch_lightning {pl.__version__}, Albumentation {A.__version__}")
if torch.cuda.is_available(): monitor = GPUStatMonitor(delay=1)

# Cell
class RetinaNetModule(LightningModule):

    def __init__(self, lbl2name:dict={}, img_sz=128, lr:float=1e-2):
        LightningModule.__init__(self)
        self.model = retinanet_resnet50_fpn(pretrained=False, num_classes=len(lbl2name)+1, pretrained_backbone=True)
        self.img_sz = img_sz
        self.lr = lr

        # Hacked to avoid model builtin call to GeneralizedRCNNTransform.normalize() as already done in augmentation pipeline
        def noop_normalize(image): return image

        # Hacked to avoid model builtin call to GeneralizedRCNNTransform.resize() as already done in augmentation pipeline
        def noop_resize(image, target): return image, target

        # HACK!! IceVision does this too!
        self.model.transform.normalize = noop_normalize
        self.model.transform.resize = noop_resize

    def freeze_head(self):
        for param in self.model.head.parameters():
            param.requires_grad = False

    def unfreeze_head(self):
        for param in self.model.head.parameters():
            param.requires_grad = True

    def freeze_backbone(self):
        for param in self.model.backbone.parameters():
            param.requires_grad = False

    def unfreeze_backbone(self):
        for param in self.model.backbone.parameters():
            param.requires_grad = True

    def training_step(self, train_batch, batch_idx):
#         print('Entering training_step')
        self.model.cuda()
        xs, ys = train_batch
        self.model.train()
        for y in ys:
            if len(y.get('boxes',[])) <= 0:
                print(f"Warning: Y has no boxes! {y}")
        losses = self.model.forward(xs, ys)
        loss = sum(losses.values())
#         print(f'Exiting training_step, returning {loss}')
        return loss # has 2 types of losses: classification, bbox_regression

    def avg_acc(self, preds, targets):
        metrics = []
        for p,t in zip(preds, targets):
            metrics.append(calc_wavg_F1(p, t, .5, .5))
        return sum(metrics)/len(targets)

    def validation_step(self, val_batch, batch_idx):
#         print('Entering validation_step')
        self.model.cpu()
        self.model.eval()

        # turn off auto gradient for validation step
        with torch.no_grad():
            xs, ys = val_batch
            xs_cpu = [ x.cpu() for x in xs ]
            ys_cpu = [ { k:v.cpu() for k,v in y.items() } for y in ys ]
            preds = self.model.forward(xs_cpu)
            avg_acc = self.avg_acc(preds, ys_cpu)

        result = {'val_acc': avg_acc}
#         print(f'Exiting validation_step, returning {result}')
        return result

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

    def validation_epoch_end(self, outputs):
        # called at the end of the validation epoch, but gradient accumulation may result in last row being different size
        val_acc_sum = sum([ o['val_acc'] for o in outputs ])
        self.log_dict({'val_acc': val_acc_sum/len(outputs)})

    def forward(self, imgs):
        self.model.eval()
        return self.model.forward(imgs)

# Cell
def run_training(stats:CocoDatasetStats, modeldir:str, img_dir:str, resume_ckpt_fname:str=None,
                 img_sz=384, bs=12, acc=4, workers=4, head_runs=50, full_runs=200,
                 monitor='val_acc', mode='max', save_top=-1):

    retnet_model = RetinaNetModule(lbl2name=stats.lbl2name, img_sz=img_sz)

    print(f"Training with image size {img_sz}, auto learning rate, for {head_runs}+{full_runs} epochs.")
    chkpt_cb = ModelCheckpoint(
        filename='retnet-subcoco-'+str(img_sz)+'-{epoch:03d}-{'+monitor+':.3f}',
        dirpath=modeldir,
        save_last=True,
        monitor=monitor,
        mode=mode,
        save_top_k=save_top,
        verbose=True,
    )
    early_stop_cb = EarlyStopping(
       monitor=monitor,
       min_delta=0.001,
       patience=20,
       verbose=True,
       mode=mode
    )
    gpumon_cb = PyTorchGpuMonitorCallback(delay=1)
    callbacks = [early_stop_cb, gpumon_cb]
    resume_ckpt = f'{modeldir}/{resume_ckpt_fname}' if resume_ckpt_fname != None else None
    if resume_ckpt and os.path.isfile(resume_ckpt):
        try:
            print(f'Loading previously saved model: {resume_ckpt}...')
            retnet_model = FRCNN.load_from_checkpoint(resume_ckpt, lbl2name=stats.lbl2name)
        except Exception as e:
            print(f'Unexpected error loading previously saved model {resume_ckpt}: {e}')
    elif resume_ckpt:
        print(f'Failed to find {resume_ckpt}')

    # transforms for images
    bbox_aware_train_tfms=A.Compose([
        A.ShiftScaleRotate(shift_limit=.025, scale_limit=0.025, rotate_limit=9),
        A.Resize(width=img_sz, height=img_sz),
        A.HorizontalFlip(p=0.5),
        A.RGBShift(),
        A.RandomBrightnessContrast(),
        A.Blur(blur_limit=(1, 3)),
        #A.Normalize(mean=stats.chn_means/255, std=stats.chn_stds/255)
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))

    bbox_aware_val_tfms=A.Compose([
        A.Resize(width=img_sz, height=img_sz),
        #A.Normalize(mean=stats.chn_means/255, std=stats.chn_stds/255)
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))

    # train head only, since using less params, double the bs and half the grad accumulation cycle to use more GPU VRAM
    if head_runs > 0:
        head_dm = SubCocoDataModule(img_dir, stats, shuffle=False,
                                    train_transforms=bbox_aware_train_tfms, val_transforms=bbox_aware_val_tfms,
                                    bs=bs, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=head_runs, default_root_dir = 'models',
                          callbacks=callbacks, checkpoint_callback=chkpt_cb, accumulate_grad_batches=acc)
        retnet_model.freeze_backbone()
        trainer.fit(retnet_model, head_dm)

    if full_runs > 0:
        # finetune head and backbone
        full_dm = SubCocoDataModule(img_dir, stats, shuffle=False,
                                    train_transforms=bbox_aware_train_tfms, val_transforms=bbox_aware_val_tfms,
                                    bs=bs, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=full_runs, default_root_dir = 'models',
                          callbacks=callbacks, checkpoint_callback=chkpt_cb, accumulate_grad_batches=acc)
        retnet_model.unfreeze_backbone()
        trainer.fit(retnet_model, full_dm)

    last_model_fpath=Path(chkpt_cb.last_model_path)
    saved_last_model_fpath=str(last_model_fpath.parent/f'retnet-subcoco-{img_sz}-last')+last_model_fpath.suffix
    os.rename(str(last_model_fpath), saved_last_model_fpath)

    return retnet_model, saved_last_model_fpath

# Cell
def save_final(retnet_model, model_save_path):
    torch.save(retnet_model.model.state_dict(), model_save_path)