# AUTOGENERATED! DO NOT EDIT! File to edit: 30_subcoco_frcnn_lightning.ipynb (unless otherwise specified).

__all__ = ['FRCNN', 'run_training', 'save_final']

# Cell
import cv2, json, os, requests, sys, tarfile, torch, torchvision
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import pickle
import random
import torch.nn.functional as F
import torch.multiprocessing

from collections import defaultdict
from functools import reduce
from IPython.utils import io
from pathlib import Path
from PIL import Image
from PIL import ImageStat

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

from torch import nn
from torch import optim
from torch.nn.modules import module
from torch.utils.data import DataLoader, random_split

from torchvision import transforms
from torchvision.datasets import CocoDetection
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

from tqdm import tqdm
from typing import Hashable, List, Tuple, Union

# Cell

import albumentations as A
import pytorch_lightning as pl
from albumentations.pytorch import ToTensorV2
from gpumonitor.monitor import GPUStatMonitor
from gpumonitor.callbacks.lightning import PyTorchGpuMonitorCallback
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning import LightningDataModule, LightningModule, Trainer
from pytorch_lightning.core.step_result import TrainResult
from .subcoco_utils import *
from .subcoco_lightning_utils import *

torch.multiprocessing.set_sharing_strategy('file_system')
print(f"Python ver {sys.version}, torch {torch.__version__}, torchvision {torchvision.__version__}, pytorch_lightning {pl.__version__}, Albumentation {A.__version__}")
if torch.cuda.is_available():
    monitor = GPUStatMonitor(delay=1)
else:
    print("CUDA not available!")

if is_notebook():
    from nbdev.showdoc import *

# Cell
class FRCNN(LightningModule):

    def __init__(self, num_classes:int=1, img_sz=128, lr:float=1e-3, test:bool=False):
        LightningModule.__init__(self)
        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

        # get number of input features of classifier
        self.in_features = self.model.roi_heads.box_predictor.cls_score.in_features

        #refit another head
        self.num_classes = num_classes
        # replace the pre-trained head with a new one, which is trainable
        self.model.roi_heads.box_predictor = FastRCNNPredictor(self.in_features, self.num_classes+1)
        self.img_sz = img_sz
        self.lr = lr
        self.test = test

        # Hacked to avoid model builtin call to GeneralizedRCNNTransform.normalize() as done in augmentation
        def noop_normalize(image): return image

        # Hacked to avoid model builtin call to GeneralizedRCNNTransform.resize() as done in augmentation
        def noop_resize(image, target): return image, target

        # HACK!! IceVision does this too!
        self.model.transform.normalize = noop_normalize
        self.model.transform.resize = noop_resize

        self.freeze()
        self.unfreeze_head()

    def _set_grad(self, mod:module, requires_grad:bool=True):
        for param in mod.parameters():
            param.requires_grad = requires_grad

    def freeze_head(self):
        self._set_grad(self.model.roi_heads, requires_grad=False)

    def unfreeze_head(self):
        self._set_grad(self.model.roi_heads, requires_grad=True)

    def freeze_backbone(self):
        self._set_grad(self.model.backbone, requires_grad=False)

    def unfreeze_backbone(self):
        self._set_grad(self.model.backbone, requires_grad=True)

    def freeze_batchnorm(self):
        for m in self.model.modules():
            if type(m) is torch.nn.BatchNorm2d:
                self._set_grad(m, requires_grad=False)

    def unfreeze_batchnorm(self):
        for m in self.model.modules():
            if type(m) is torch.nn.BatchNorm2d:
                self._set_grad(m, requires_grad=True)

    # FRCNN is tricky, in training model the model takes X & Y to produce loss
    def training_step(self, train_batch, batch_idx):
        if self.test: print('Entering training_step')
        self.model.cuda()
        self.model.train()
        xs, ys = train_batch
        losses = self.model.forward(xs, ys)
        # has 4 types of losses: loss_classifier loss_box_reg loss_objectness loss_rpn_box_reg
        loss = sum(losses.values())
        if self.test: print(f'Exiting training_step, returning {loss}')
        return loss

    def metrics(self, preds, targets):
        metrics = torch.zeros((min(len(preds), len(targets)), 2))
        for i, (p,t) in enumerate(zip(preds, targets)):
            metrics[i,0] = calc_wavg_F1(p, t, .5, .5)
            metrics[i,1] = SubCocoWrapper(p, t, self.img_sz, self.img_sz).metrics()[0]
        return metrics

    def validation_step(self, val_batch, batch_idx):
        if self.test: print('Entering validation_step')
        self.model.cpu()
        self.model.eval()

        # turn off auto gradient for validation step
        with torch.no_grad():
            xs, ys = val_batch
            xs_cpu = [ x.cpu() for x in xs ]
            ys_cpu = [ { k:v.cpu() for k,v in y.items() } for y in ys ]
            preds = self.model(xs_cpu)
            metrics = self.metrics(preds, ys)

            self.model.train()
            losses = self.model(xs_cpu, ys_cpu)

        result = {'val_acc': metrics[:,0].mean(), 'val_coco': metrics[:,1].mean(), 'val_loss': sum(losses.values())}
        if self.test: print(f'Exiting validation_step, returning {result}')
        return result

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

    def validation_epoch_end(self, outputs):
        if self.test: print('Entering validation_epoch_end')
        tensorboard_logs ={}
        avg_acc = sum([ o['val_acc'] for o in outputs ])/len(outputs)
        tensorboard_logs['val_acc'] = avg_acc

        avg_coco = sum([ o['val_coco'] for o in outputs ])/len(outputs)
        tensorboard_logs['val_coco'] = avg_coco

        avg_loss = sum([ o['val_loss'] for o in outputs ])/len(outputs)
        tensorboard_logs['val_loss'] = avg_loss
        print(f"Epoch end avg val_acc = {avg_acc} (F1 @ IoU>.5, Score>.5), avg val_coco = {avg_coco}, avg val_loss = {avg_loss}")
        result = {'val_acc': avg_acc, 'val_coco': avg_coco, 'val_loss': avg_loss, 'logs': tensorboard_logs}
        if self.test: print(f'Exiting validation_epoch_end, returning {result}')
        self.log_dict(result)

    # forward takes only X, so need to put FRCNN in eval mode
    def forward(self, xs):
        self.model.eval()
        return self.model(xs)

# Cell
def run_training(stats:CocoDatasetStats, modeldir:str, img_dir:str, resume_ckpt_fname:str=None,
                 img_sz=384, bs=12, acc=4, workers=4, head_runs=50, full_runs=200,
                 monitor='val_acc', mode='max', save_top=-1, test=False
                ):

    resume_ckpt = f'{modeldir}/{resume_ckpt_fname}' if resume_ckpt_fname != None else None

    if resume_ckpt and os.path.isfile(resume_ckpt):
        try:
            print(f'Loading previously saved model: {resume_ckpt}...')
            frcnn_model = FRCNN.load_from_checkpoint(resume_ckpt, num_classes=len(stats.lbl2name), img_sz=img_sz)
        except Exception as e:
            print(f'Unexpected error loading previously saved model {resume_ckpt}: {e}')
            frcnn_model = FRCNN(num_classes=len(stats.lbl2name), img_sz=img_sz, test=test)
    else:
        if resume_ckpt: print(f'Failed to find {resume_ckpt}')
        frcnn_model = FRCNN(num_classes=len(stats.lbl2name), img_sz=img_sz, test=test)

    print(f"Training with image size {img_sz}, auto learning rate, for {head_runs}+{full_runs} epochs.")
    head_chkpt_cb = ModelCheckpoint(
        filename='FRCNN-HEAD-subcoco-'+str(img_sz)+'-{epoch:03d}-{'+monitor+':.3f}',
        dirpath=modeldir,
        save_last=True,
        monitor=monitor,
        mode=mode,
        save_top_k=save_top,
        verbose=True,
    )
    full_chkpt_cb = ModelCheckpoint(
        filename='FRCNN-FULL-subcoco-'+str(img_sz)+'-{epoch:03d}-{'+monitor+':.3f}',
        dirpath=modeldir,
        save_last=True,
        monitor=monitor,
        mode=mode,
        save_top_k=save_top,
        verbose=True,
    )
    early_stop_cb = EarlyStopping(
       monitor=monitor,
       min_delta=0.001,
       patience=5,
       verbose=True,
       mode=mode
    )
    gpumon_cb = PyTorchGpuMonitorCallback(delay=1)
    callbacks = [early_stop_cb, gpumon_cb]

    # transforms for images
    bbox_aware_train_tfms=A.Compose([
        A.ShiftScaleRotate(shift_limit=.025, scale_limit=0.025, rotate_limit=9),
        A.Resize(width=img_sz, height=img_sz),
        A.HorizontalFlip(p=0.5),
        A.RGBShift(),
        A.RandomBrightnessContrast(),
        A.Blur(blur_limit=(1, 3)),
        #A.Normalize(), #mean=stats.chn_means/255, std=stats.chn_stds/255)
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))

    bbox_aware_val_tfms=A.Compose([
        A.Resize(width=img_sz, height=img_sz),
        #A.Normalize(), #mean=stats.chn_means/255, std=stats.chn_stds/255)
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))

    # train head only, since using less params, double the bs and half the grad accumulation cycle to use more GPU VRAM
    if head_runs > 0:
        head_dm = SubCocoDataModule(img_dir, stats,
                                    train_transforms=bbox_aware_train_tfms, val_transforms=bbox_aware_val_tfms,
                                    bs=bs*2, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=head_runs, default_root_dir = 'models',
                          callbacks=callbacks, checkpoint_callback=head_chkpt_cb, accumulate_grad_batches=int(acc//2))
        frcnn_model.unfreeze_head()
        frcnn_model.freeze_backbone()
        frcnn_model.unfreeze_batchnorm()
        trainer.fit(frcnn_model, head_dm)

    if full_runs > 0:
        frcnn_model.unfreeze() # allow finetuning of the backbone
        # finetune head and backbone
        full_dm = SubCocoDataModule(img_dir, stats,
                                    train_transforms=bbox_aware_train_tfms, val_transforms=bbox_aware_val_tfms,
                                    bs=bs, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=full_runs, default_root_dir = 'models',
                          callbacks=callbacks, checkpoint_callback=full_chkpt_cb, accumulate_grad_batches=acc)
        frcnn_model.unfreeze_head()
        frcnn_model.unfreeze_backbone()
        frcnn_model.unfreeze_batchnorm()
        trainer.fit(frcnn_model, full_dm)

    if full_runs > 0:
        last_model_fpath=Path(full_chkpt_cb.last_model_path)
        saved_last_model_fpath=str(last_model_fpath.parent/f'FRCNN-subcoco-{img_sz}-last')+last_model_fpath.suffix
        os.rename(str(last_model_fpath), saved_last_model_fpath)

    return frcnn_model, str(saved_last_model_fpath)

# Cell
def save_final(frcnn_model, model_save_path):
    torch.save(frcnn_model.model.state_dict(), model_save_path)