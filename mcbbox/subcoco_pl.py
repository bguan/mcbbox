# AUTOGENERATED! DO NOT EDIT! File to edit: 30_subcoco_pl.ipynb (unless otherwise specified).

__all__ = ['run_training', 'save_final']

# Cell
import json, os, requests, sys, tarfile, torch, torchvision
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import pickle
import pytorch_lightning as pl
import torch.nn.functional as F
import torch.multiprocessing

from collections import defaultdict
from IPython.utils import io
from pathlib import Path
from PIL import Image
from PIL import ImageStat

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import LightningDataModule, LightningModule, Trainer
from pytorch_lightning.core.step_result import TrainResult

from torch import nn
from torch import optim
from torch.utils.data import DataLoader, random_split

from torchvision import transforms
from torchvision.datasets import CocoDetection
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

from tqdm import tqdm

from .subcoco_utils import *

torch.multiprocessing.set_sharing_strategy('file_system')

# Cell
def run_training(stats:CocoDatasetStats, img_dir:str, img_sz=384, bs=12, acc=4, workers=4, head_runs=50, full_runs=200):

    frcnn_model = FRCNN(lbl2name=stats.lbl2name)

    print(f"Training with image size {img_sz}, auto learning rate, for {head_runs}+{full_runs} epochs.")
    chkpt_cb = ModelCheckpoint(
        filepath="FRCNN-subcoco-{epoch}-{int(val_acc*100):d}-pcnt.ckpt",
        save_last=True,
        monitor='val_acc',
        mode='max',
        save_top_k=3,
        verbose=True,
    )

    # train head only, since using less params, double the bs and half the grad accumulation cycle to use more GPU VRAM
    if head_runs > 0:
        head_dm = SubCocoDataModule(img_dir, stats, resize=(img_sz,img_sz), bs=bs*2, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=head_runs, default_root_dir = 'models',
                          checkpoint_callback=chkpt_cb, accumulate_grad_batches=max(1,int(acc//2)))
        trainer.fit(frcnn_model, head_dm)

    if full_runs > 0:
        frcnn_model.unfreeze() # allow finetuning of the backbone
        # finetune head and backbone
        full_dm = SubCocoDataModule(img_dir, stats, resize=(img_sz,img_sz), bs=bs, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=full_runs, default_root_dir = 'models',
                          checkpoint_callback=chkpt_cb, accumulate_grad_batches=max(1,acc))
        trainer.fit(frcnn_model, full_dm)

    return frcnn_model

# Cell
def save_final(frcnn_model, model_save_path):
    torch.save(frcnn_model.model.state_dict(), model_save_path)