# AUTOGENERATED! DO NOT EDIT! File to edit: 40_subcoco_retinanet_lightning.ipynb (unless otherwise specified).

__all__ = ['SubCocoDataset', 'SubCocoDataModule', 'RetinaNetModule', 'run_training', 'save_final']

# Cell
import cv2, json, os, requests, sys, tarfile, torch, torchvision
import albumentations as A
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import pickle
import random
import torch.nn.functional as F
import torch.multiprocessing

from albumentations.pytorch import ToTensorV2
from collections import defaultdict
from functools import reduce
from IPython.utils import io
from pathlib import Path
from PIL import Image
from PIL import ImageStat

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

from torch import nn
from torch import optim
from torch.utils.data import DataLoader, random_split
from torchvision.models.detection import RetinaNet, retinanet_resnet50_fpn

from torchvision import transforms

from tqdm import tqdm
from typing import Hashable, List, Tuple, Union

torch.multiprocessing.set_sharing_strategy('file_system')

# Cell
import pytorch_lightning as pl
from gpumonitor.monitor import GPUStatMonitor
from gpumonitor.callbacks.lightning import PyTorchGpuMonitorCallback
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning import LightningDataModule, LightningModule, Trainer
from pytorch_lightning.core.step_result import TrainResult
from .subcoco_utils import *

print(f"Python ver {sys.version}, torch {torch.__version__}, torchvision {torchvision.__version__}, pytorch_lightning {pl.__version__}, Albumentation {A.__version__}")
if torch.cuda.is_available(): monitor = GPUStatMonitor(delay=1)

# Cell
class SubCocoDataset(torchvision.datasets.VisionDataset):
    """
    Simulate what torchvision.CocoDetect() returns for target given fastai's coco subsets
    Args:
        root (string): Root directory where images are downloaded to.
        stats (CocoDatasetStats):
    """

    def __init__(self, root:str, stats:CocoDatasetStats, img_ids:list=[], bbox_aware_tfms:callable=None):
        super(SubCocoDataset, self).__init__(root)
        self.stats = stats
        self.img_ids = []
        n_missing = 0
        for img_id in img_ids:
            img_fname = stats.img2fname[img_id]
            if not os.path.isfile(stats.img_dir/img_fname):
                n_missing += 1
            elif stats.img2sz.get(img_id, None) is None:
                n_missing += 1
            else:
                self.img_ids.append(img_id)
        if n_missing > 0 : print(f'Warning: {n_missing} image files are missing!!!')
        self.bbox_aware_tfms = bbox_aware_tfms

    def __getitem__(self, index):
        """
        Args:
            index (int): Index
        Returns:
            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.
        """
        img_id = self.img_ids[index] if index < len(self.img_ids) else 0
        img_fname = self.stats.img2fname.get(img_id, None)
        if img_id is None or img_fname ==None:
            return (None, None)
        img_fpath = os.path.join(self.root, img_fname)
        img_w, img_h = self.stats.img2sz.get(img_id, (1,1))
        target = { 'boxes': [], 'labels': [], 'image_id': img_id, 'width': img_h, 'height': img_h, 'areas': [], 'iscrowds': 0, 'ids': [] }
        count = 0
        lbs = self.stats.img2lbs.get(img_id,[])
        for l, x, y, w, h in lbs:
            count += 1
            target['boxes'].append([x, y, x+w, y+h]) # FRCNN wants x1,y1,x2,y2 format!
            target['labels'].append(l)
            target['areas'].append(w*h)
            anno_id = img_id*1000 + count
            target['ids'].append(anno_id)

        img = cv2.imread(img_fpath)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if self.bbox_aware_tfms is not None:
            transformed = self.bbox_aware_tfms(image=img, bboxes=target['boxes'], class_labels=target['labels'])
            img = transformed['image']
            target['boxes'] = transformed['bboxes']

        for k, v in target.items():
            target[k] = torch.tensor(v, dtype=(torch.float if k in ['boxes', 'width', 'height', 'areas'] else torch.long))

        img = torch.from_numpy(img/255.0).float().permute(2, 0, 1)
        return img, target

    def __len__(self):
        return len(self.img_ids)

# Cell
class SubCocoDataModule(LightningDataModule):

    def __init__(self, root, stats, bs=32, workers=4, split_ratio=0.8, shuffle=True,
                 train_transforms=None, val_transforms=None):
        super().__init__(train_transforms=train_transforms, val_transforms=val_transforms)
        self.dir = root
        self.bs = bs
        self.workers = workers
        self.stats = stats
        self.split_ratio = split_ratio
        self.shuffle = shuffle

        num_items = stats.num_imgs
        num_train = int(self.split_ratio*num_items)
        img_ids = list(stats.img2sz.keys())
        if shuffle: random.shuffle(img_ids)

        train_img_ids = img_ids[:num_train]
        val_img_ids = img_ids[num_train:]

        self.train = SubCocoDataset(self.dir, self.stats, img_ids=train_img_ids, bbox_aware_tfms=train_transforms)
        self.val = SubCocoDataset(self.dir, self.stats, img_ids=val_img_ids, bbox_aware_tfms=val_transforms)

    def collate_fn(self, batch):
        return tuple(zip(*batch))

    def train_dataloader(self):
        return DataLoader(self.train, batch_size=self.bs, num_workers=self.workers, collate_fn=self.collate_fn, shuffle=self.shuffle)

    def val_dataloader(self):
        return DataLoader(self.val, batch_size=self.bs, num_workers=self.workers, collate_fn=self.collate_fn, shuffle=False)

# Cell
class RetinaNetModule(LightningModule):

    def __init__(self, lbl2name:dict={}, img_sz=128, lr:float=1e-2):
        LightningModule.__init__(self)
        self.model = retinanet_resnet50_fpn(pretrained=False, num_classes=len(lbl2name)+1, pretrained_backbone=True)
        self.img_sz = img_sz
        self.lr = lr

        # Hacked to avoid model builtin call to GeneralizedRCNNTransform.normalize() as already done in augmentation pipeline
        def noop_normalize(image): return image

        # Hacked to avoid model builtin call to GeneralizedRCNNTransform.resize() as already done in augmentation pipeline
        def noop_resize(image, target): return image, target

        # HACK!! IceVision does this too!
        self.model.transform.normalize = noop_normalize
        self.model.transform.resize = noop_resize

    def freeze_head(self):
        for param in self.model.head.parameters():
            param.requires_grad = False

    def unfreeze_head(self):
        for param in self.model.head.parameters():
            param.requires_grad = True

    def freeze_backbone(self):
        for param in self.model.backbone.parameters():
            param.requires_grad = False

    def unfreeze_backbone(self):
        for param in self.model.backbone.parameters():
            param.requires_grad = True

    def training_step(self, train_batch, batch_idx):
#         print('Entering training_step')
        self.model.cuda()
        xs, ys = train_batch
        self.model.train()
        loss = sum(self.model.forward(xs, ys).values())
#         print(f'Exiting training_step, returning {loss}')
        return loss # has 2 types of losses: classification, bbox_regression

    def avg_acc(self, preds, targets):
        metrics = []
        for p,t in zip(preds, targets):
            metrics.append(calc_wavg_F1(p, t, .5, .5))
        return sum(metrics)/len(targets)

    def validation_step(self, val_batch, batch_idx):
#         print('Entering validation_step')
        self.model.cpu()
        self.model.eval()

        # turn off auto gradient for validation step
        with torch.no_grad:
            xs, ys = val_batch
            xs_cpu = [ x.cpu() for x in xs ]
            ys_cpu = [ { k:v.cpu() for k,v in y.items() } for y in ys ]
            preds = self.model.forward(xs_cpu)
            avg_acc = self.avg_acc(preds, ys_cpu)

        result = {'val_acc': avg_acc}
#         print(f'Exiting validation_step, returning {result}')
        return result

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

    def validation_epoch_end(self, outputs):
        # called at the end of the validation epoch, but gradient accumulation may result in last row being different size
        val_acc_sum = sum([ o['val_acc'] for o in outputs ])
        self.log_dict({'val_acc': val_acc_sum/len(outputs)})

# Cell
def run_training(stats:CocoDatasetStats, modeldir:str, img_dir:str, resume_ckpt_fname:str=None,
                 img_sz=384, bs=12, acc=4, workers=4, head_runs=50, full_runs=200,
                 monitor='val_acc', mode='max', save_top=-1):

    retnat_model = RetinaNetModule(lbl2name=stats.lbl2name, img_sz=img_sz)

    print(f"Training with image size {img_sz}, auto learning rate, for {head_runs}+{full_runs} epochs.")
    chkpt_cb = ModelCheckpoint(
        filename='retnat-subcoco-'+str(img_sz)+'-{epoch:03d}-{'+monitor+':.3f}',
        dirpath=modeldir,
        save_last=True,
        monitor=monitor,
        mode=mode,
        save_top_k=save_top,
        verbose=True,
    )
    early_stop_cb = EarlyStopping(
       monitor=monitor,
       min_delta=0.001,
       patience=5,
       verbose=True,
       mode=mode
    )
    gpumon_cb = PyTorchGpuMonitorCallback(delay=1)
    callbacks = [early_stop_cb, gpumon_cb]
    resume_ckpt = f'{modeldir}/{resume_ckpt_fname}' if resume_ckpt_fname != None else None
    if resume_ckpt and os.path.isfile(resume_ckpt):
        try:
            print(f'Loading previously saved model: {resume_ckpt}...')
            retnat_model = FRCNN.load_from_checkpoint(resume_ckpt, lbl2name=stats.lbl2name)
        except Exception as e:
            print(f'Unexpected error loading previously saved model {resume_ckpt}: {e}')
    elif resume_ckpt:
        print(f'Failed to find {resume_ckpt}')

    # transforms for images
    bbox_aware_train_tfms=A.Compose([
        A.ShiftScaleRotate(shift_limit=.025, scale_limit=0.025, rotate_limit=9),
        A.Resize(width=img_sz, height=img_sz),
        A.HorizontalFlip(p=0.5),
        A.RGBShift(),
        A.RandomBrightnessContrast(),
        A.Blur(blur_limit=(1, 3)),
        #A.Normalize(mean=stats.chn_means/255, std=stats.chn_stds/255)
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))

    bbox_aware_val_tfms=A.Compose([
        A.Resize(width=img_sz, height=img_sz),
        #A.Normalize(mean=stats.chn_means/255, std=stats.chn_stds/255)
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))

    # train head only, since using less params, double the bs and half the grad accumulation cycle to use more GPU VRAM
    if head_runs > 0:
        head_dm = SubCocoDataModule(img_dir, stats, shuffle=False,
                                    train_transforms=bbox_aware_train_tfms, val_transforms=bbox_aware_val_tfms,
                                    bs=bs, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=head_runs, default_root_dir = 'models',
                          callbacks=callbacks, checkpoint_callback=chkpt_cb, accumulate_grad_batches=acc)
        retnat_model.freeze_backbone()
        trainer.fit(retnat_model, head_dm)

    if full_runs > 0:
        # finetune head and backbone
        full_dm = SubCocoDataModule(img_dir, stats, shuffle=False,
                                    train_transforms=bbox_aware_train_tfms, val_transforms=bbox_aware_val_tfms,
                                    bs=bs, workers=workers)
        trainer = Trainer(gpus=1, auto_lr_find=True, max_epochs=full_runs, default_root_dir = 'models',
                          callbacks=callbacks, checkpoint_callback=chkpt_cb, accumulate_grad_batches=acc)
        retnat_model.unfreeze_backbone()
        trainer.fit(retnat_model, full_dm)

    last_model_fpath=Path(chkpt_cb.last_model_path)
    saved_last_model_fpath=str(last_model_fpath.parent/f'FRCNN-subcoco-{img_sz}-last')+last_model_fpath.suffix
    os.rename(str(last_model_fpath), saved_last_model_fpath)

    return retnat_model, saved_last_model_fpath

# Cell
def save_final(retnat_model, model_save_path):
    torch.save(retnat_model.model.state_dict(), model_save_path)